{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "voc2012.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/dkatsios/semantic_segmentation/blob/master/voc2012.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "5nyI7k-GbaLK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "colab = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "43IKGALxGDr4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from shutil import unpack_archive\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import Image\n",
        "import PIL\n",
        "from keras.optimizers import Adam\n",
        "from time import time\n",
        "if colab:\n",
        "  from google.colab import files\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "foYYmKrmKCEq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download VOC 2012 dataset"
      ]
    },
    {
      "metadata": {
        "id": "Y-njzkyzDplH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if colab:\n",
        "#   %rm -r /content/semantic_segmentation\n",
        "#   %mkdir /content/semantic_segmentation\n",
        "#   %cd /content/semantic_segmentation/\n",
        "#   !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
        "#   unpack_archive('VOCtrainval_11-May-2012.tar', './')\n",
        "#   %rm VOCtrainval_11-May-2012.tar\n",
        "  wdir = '/content/semantic_segmentation'\n",
        "else:\n",
        "  wdir = 'E:/Data_Files/Workspaces/PyCharm/semantic_segmentation/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w8QH4XWEy1YZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4319d841-a0a6-443b-b49a-830b03f3a0cd"
      },
      "cell_type": "code",
      "source": [
        "%cd {wdir}/VOCdevkit/VOC2012\n",
        "!ls"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/semantic_segmentation/VOCdevkit/VOC2012\n",
            "Annotations  ImageSets\tJPEGImages  SegmentationClass  SegmentationObject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LNq2KWv31kW2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imgs_folder = wdir + '/VOCdevkit/VOC2012/JPEGImages/'\n",
        "classes_folder = wdir + '/VOCdevkit/VOC2012/SegmentationClass/'\n",
        "train_list_path = wdir + '/VOCdevkit/VOC2012/ImageSets/Segmentation/train.txt'\n",
        "val_list_path = wdir + '/VOCdevkit/VOC2012/ImageSets/Segmentation/val.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c32_Y5H9KQOe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import helpers and Model files"
      ]
    },
    {
      "metadata": {
        "id": "iRS1BasOH9_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f9b23151-f6ab-4fb7-f5d3-cea4adce28c0"
      },
      "cell_type": "code",
      "source": [
        "if colab:\n",
        "    %cd {wdir}\n",
        "    %rm -r {wdir}/semantic_segmentation/\n",
        "    !git clone https://github.com/dkatsios/semantic_segmentation.git\n",
        "    %cd {wdir}/semantic_segmentation\n",
        "else:\n",
        "    %cd {wdir}\n",
        "%ls"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/semantic_segmentation\n",
            "Cloning into 'semantic_segmentation'...\n",
            "remote: Counting objects: 63, done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 63 (delta 35), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (63/63), done.\n",
            "/content/semantic_segmentation/semantic_segmentation\n",
            "AtrusUnet_2.ipynb  README.md              voc2012.ipynb\n",
            "AtrusUnet.ipynb    voc2012_helpers.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "guNQw-HoICFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7020fc15-525c-4976-b022-ac0242bced86"
      },
      "cell_type": "code",
      "source": [
        "!pip install import_ipynb\n",
        "import import_ipynb\n",
        "from voc2012_helpers import *\n",
        "# from AtrusUnet import  *"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5ZUnCWdeIhud",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, Input, Concatenate, Deconv2D, Lambda, \\\n",
        "ZeroPadding2D, SeparableConv2D, BatchNormalization, Dropout, MaxPooling2D\n",
        "from keras.regularizers import l1, l2\n",
        "from keras.models import Model\n",
        "\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wGdEw1uZRWXX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AtrousUnet:\n",
        "  def __init__(self, img_shape, filters, out_channels,\n",
        "               steps, out_resized_levels, kernel_sizes=None, dilation_rates=None, use_depthwise=False,\n",
        "               use_max_pooling=True, use_regularizers=True, pre_resized=False, dropout_rate=0.4):\n",
        "    self.img_shape = img_shape\n",
        "    self.filters = filters\n",
        "    self.out_channels = out_channels\n",
        "    self.steps = steps\n",
        "    self.out_resized_levels = out_resized_levels\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.use_max_pooling = use_max_pooling\n",
        "    self.use_regularizers = use_regularizers\n",
        "    self.pre_resized = pre_resized\n",
        "    self.kernel_sizes = kernel_sizes if kernel_sizes is not None else [2, 3, 5, 7]\n",
        "    self.dilation_rates = dilation_rates if dilation_rates is not None else [1, 2, 3]\n",
        "    self.kern_reg, self.act_reg = (l2(), l1()) if self.use_regularizers else (None, None)\n",
        "    self.conv = SeparableConv2D if use_depthwise else Conv2D\n",
        "    self.deconv = Deconv2D  # SeparableConvolution2D if use_depthwise else Deconv2D \n",
        "    \n",
        "    assert 2 ** self.steps <= np.min(img_shape[:2]) and \\\n",
        "    self.out_resized_levels <= self.steps\n",
        "    \n",
        "  def resize_img(self, input_tensor):\n",
        "    return tf.image.resize_images(input_tensor[0],\n",
        "                                  input_tensor[1].shape[1:-1])\n",
        "  \n",
        "  def check_shape(self, x):\n",
        "    x_shape = K.int_shape(x)[1:-1]\n",
        "    if x_shape == self.current_shape:\n",
        "      return x\n",
        "    \n",
        "    dr = self.current_shape[0] - x_shape[0]\n",
        "    if dr == 0:\n",
        "      r_pad = 0, 0\n",
        "    elif dr % 2 == 0:\n",
        "      r_pad = dr // 2, dr // 2\n",
        "    else:\n",
        "      r_pad = dr // 2, dr // 2 + 1\n",
        "    \n",
        "    dc = self.current_shape[1] - x_shape[1]\n",
        "    if dc == 0:\n",
        "      c_pad = 0, 0\n",
        "    elif dc % 2 == 0:\n",
        "      c_pad = dc // 2, dc // 2\n",
        "    else:\n",
        "      c_pad = dc // 2, dc // 2 + 1\n",
        "      \n",
        "    assert np.abs(np.array([r_pad, c_pad])).all() <= 1\n",
        "    if dr < 0 or dc < 0:\n",
        "      r = -r_pad[0], x_shape[0] + r_pad[1]\n",
        "      c = -c_pad[0], x_shape[1] + c_pad[1]\n",
        "      x = Lambda(lambda x: x[:,r[0]:r[1],c[0]:c[1],:])(x)\n",
        "    else:\n",
        "      x = ZeroPadding2D((r_pad, c_pad))(x)\n",
        "    return x\n",
        "  \n",
        "  def down_sampling_block(self, input_img, input_tensor, ind):\n",
        "    \"\"\"\n",
        "    The downsampling block takes as input the original image (input_img)\n",
        "    and the output of the previous downsampling block (input_tensor).\n",
        "    Steps:\n",
        "      - downsampling (kernel 2x2, strides 2) of 'input_tensor' to half its shape to 'down_sampled'\n",
        "      - for each kernel size:\n",
        "        - convolution with this kernel size, strides 1 and dilation 2\n",
        "        - downsampling (convolution) with kernel 2x2 and strides 2\n",
        "      - concatenation of the last one with 'down_sampled' to 'down_sampled'\n",
        "      - resize of input_img to the same size as 'down_sampled' to 'resized_img'\n",
        "      - convolution of 'down_sampled'  with kernel 1x1\n",
        "      - concatenation of 'down_sampled' with 'resized_img' to total channels 2*filters\n",
        "      - batch normalization.\n",
        "    \"\"\"\n",
        "    ###############\n",
        "    kern_reg, act_reg = self.kern_reg, self.act_reg\n",
        "    dilated = []\n",
        "    for k in self.kernel_sizes:\n",
        "      for d in self.dilation_rates:\n",
        "        x = self.conv(self.filters // 2, (k, k), strides=(1, 1), activation='relu',\n",
        "#                       depthwise_regularizer=kern_reg, pointwise_regularizer=kern_reg, activity_regularizer=act_reg,\n",
        "                      padding='same', dilation_rate=(d, d))(input_tensor)\n",
        "        dilated.append(x)\n",
        "    ###############        \n",
        "    concatenated = Concatenate()([*dilated])\n",
        "    if self.use_max_pooling:\n",
        "      shortened = Conv2D(2 * self.filters, (1, 1), activation='relu',\n",
        "                         kernel_regularizer=kern_reg, activity_regularizer=act_reg,\n",
        "                         padding='same')(concatenated)\n",
        "      down_sampled = MaxPooling2D()(shortened)\n",
        "    else:\n",
        "      down_sampled = self.conv(2 * self.filters, (3, 3), strides=(2, 2), activation='relu',\n",
        "                               depthwise_regularizer=kern_reg, pointwise_regularizer=kern_reg,\n",
        "                               activity_regularizer=act_reg,\n",
        "                               padding='same')(concatenated)\n",
        "    \n",
        "#     down_sampled = BatchNormalization()(down_sampled)\n",
        "    down_sampled = Dropout(self.dropout_rate)(down_sampled)\n",
        "    ###############\n",
        "    if self.pre_resized:\n",
        "      self.current_shape = K.int_shape(down_sampled)[1:-1]\n",
        "      resized_img = input_img\n",
        "      resized_img = self.check_shape(resized_img)\n",
        "    else:\n",
        "      resized_img = Lambda(self.resize_img)([input_img, down_sampled])\n",
        "    merged = Concatenate()([down_sampled, resized_img])\n",
        "    ###############\n",
        "    return merged\n",
        "  \n",
        "  def up_sampling_block(self, down, same, index):\n",
        "    \"\"\"\n",
        "    The upsampling block takes as input the output of the previous upsampling block (down)\n",
        "    and the output of the downsampling block of the same level (same).\n",
        "    Steps:\n",
        "      - upsampling (deconvolution) of 'down' to the shape of 'same'\n",
        "      - concatenation of the previous one with the 'same' to the 'concatenated'\n",
        "      - convolution of the 'concatenated' to same shape and standard filters\n",
        "      - batch normalization of previous\n",
        "      - optionally convolves to 1x1 filters and output channels for intermediate loss.\n",
        "    \"\"\"\n",
        "    self.current_shape = K.int_shape(same)[1:-1]\n",
        "    kern_reg, act_reg = self.kern_reg, self.act_reg\n",
        "    upsampled = []\n",
        "    out_name = 'prediction' if index == (self.steps+1) else 'resized_%d' % (self.steps + 1 - index)\n",
        "    \n",
        "    for k in self.kernel_sizes:\n",
        "      if k == 1 and len(self.kernel_sizes) != 1:\n",
        "        continue\n",
        "      \n",
        "      x = self.deconv(self.filters // 2, (k, k), activation='relu',\n",
        "#                       kernel_regularizer=kern_reg, activity_regularizer=act_reg,\n",
        "                      strides=(2, 2), padding='same')(down)\n",
        "      x = self.check_shape(x)\n",
        "      upsampled.append(x)\n",
        "    \n",
        "    concatenated = Concatenate()([*upsampled, same])\n",
        "    up_sampled = Conv2D(2 * self.filters, (1, 1), activation='relu',\n",
        "                        kernel_regularizer=kern_reg, activity_regularizer=act_reg,\n",
        "                        padding='same')(concatenated)\n",
        "    \n",
        "#     up_sampled = BatchNormalization()(up_sampled)\n",
        "    \n",
        "    out_sampled = Conv2D(self.out_channels + 1, (1, 1), activation='softmax',\n",
        "                         padding='same', name=out_name)(up_sampled)\n",
        "    \n",
        "    up_sampled = Dropout(self.dropout_rate)(up_sampled)\n",
        "    \n",
        "    return [up_sampled, out_sampled]\n",
        "  \n",
        "#   def build_toy_model(self):\n",
        "#     input_img = Input(self.img_shape)\n",
        "#     x = Conv2D(self.filters, (3, 3), activation='relu', padding='same')(input_img)\n",
        "#     x = Conv2D(self.filters, (5, 5), activation='relu', padding='same')(x)\n",
        "#     x = Conv2D(self.filters, (7, 7), activation='relu', padding='same')(x)\n",
        "#     out = Conv2D(self.out_channels + 1, (1, 1), activation='softmax', padding='same', name='prediction')(x)\n",
        "#     self.model = Model(input_img, out)\n",
        "  \n",
        "  def build_model(self):\n",
        "    \"\"\"\n",
        "    The model has the downsample stage and the upsample stage.\n",
        "    The downsample stage has the original image and n steps of the downsampling block.\n",
        "    The upsample stage has n steps of the upsampling block and the original image.\n",
        "    The loss is computed over the last result (original image shape) and m resized results (up_sampled).\n",
        "    \"\"\"\n",
        "    input_img = [Input(self.img_shape, name='or_image')]\n",
        "\n",
        "    # downsampling stage\n",
        "    down_sampled = [input_img[-1]]\n",
        "    for i in range(self.steps):\n",
        "      if self.pre_resized:\n",
        "        resized_shape = self.img_shape[0] // (2 ** (i+1)), self.img_shape[1] // (2 ** (i+1)), self.img_shape[2]\n",
        "        input_img.append(Input(resized_shape, name='resized_image_%d' % (i+1)))\n",
        "      \n",
        "      down_sampled.append(self.down_sampling_block(input_img[-1], down_sampled[i], i))\n",
        "\n",
        "    # upsampling stage\n",
        "    up_sampled = [down_sampled[-1]]\n",
        "    up_results = down_sampled[-1],\n",
        "    for i in range(2, self.steps+2):\n",
        "      up_results = self.up_sampling_block(up_results[0],\n",
        "                                          down_sampled[-i], i)\n",
        "      up_sampled.append(up_results[1])\n",
        "    \n",
        "    up_sampled = up_sampled[-(self.out_resized_levels + 1):]\n",
        "    if len(up_sampled) == 1:\n",
        "      up_sampled = up_sampled[0]\n",
        "      \n",
        "    # model\n",
        "    model = Model(input_img, up_sampled)\n",
        "    assert self.img_shape[:-1] == model.layers[-1].output_shape[1:-1]\n",
        "    \n",
        "    self.model = model\n",
        "#     self.build_toy_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GJYzkWcOKWmm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set parameters"
      ]
    },
    {
      "metadata": {
        "id": "E_UVWO53JEMZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_shape = 512, 512, 3\n",
        "filters = 128  # 256\n",
        "segmentation_classes = 21\n",
        "steps = 5\n",
        "out_resized_levels = 0\n",
        "kernel_sizes = [2, 3, 5, 7]  # [2, 3, 5, 7]\n",
        "dilation_rates = [1, 2, 3]  # [1, 2, 3]\n",
        "use_max_pooling = True\n",
        "use_depthwise = True\n",
        "pre_resized = False\n",
        "\n",
        "batch_size = 1\n",
        "epochs = 1\n",
        "val_steps = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ikAKnviJJfKg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build generators"
      ]
    },
    {
      "metadata": {
        "id": "htwoddmUJiN4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train_lists, val_lists = get_lists_from_folders(train_list_path, val_list_path, imgs_folder, classes_folder)\n",
        "\n",
        "# # train_lists = train_lists[0][:100], train_lists[1][:100]\n",
        "# # val_lists = val_lists[0][:100], val_lists[1][:100]\n",
        "\n",
        "# train_arrays = get_imgs_classes_arrays(*train_lists, img_shape)\n",
        "# val_arrays = get_imgs_classes_arrays(*val_lists, img_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SGLAFwpXnC_H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_pre_resized(batch_imgs, steps):\n",
        "  pre_resized_imgs = {'or_image': batch_imgs}\n",
        "  or_size = batch_imgs.shape[1:-1]\n",
        "  for i in range(1, steps+1):\n",
        "    key = 'resized_image_%d' % i\n",
        "    size = or_size[0] // (2 ** i), or_size[1] // (2 ** i)\n",
        "    value = np.zeros((batch_imgs.shape[0], size[0], size[1], batch_imgs.shape[3]))\n",
        "    \n",
        "    for batch in range(batch_imgs.shape[0]):\n",
        "      value[batch] = cv2.resize(batch_imgs[batch], size)\n",
        "      \n",
        "    pre_resized_imgs[key] = value\n",
        "#   for key, value in pre_resized_imgs.items():\n",
        "#     print(key, value.shape)\n",
        "  return pre_resized_imgs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oe010pfAm5Ug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def imgs_generator(rgb_imgs, num_classes, batch_size, out_resized_levels, segmentation_classes, pre_resized, steps):\n",
        "  while True:\n",
        "    inds = np.random.randint(0, rgb_imgs.shape[0], batch_size)\n",
        "    batch_imgs = rgb_imgs[inds]\n",
        "    if pre_resized:\n",
        "      batch_imgs = get_pre_resized(batch_imgs, steps)\n",
        "    batch_classes = get_resized(num_classes[inds], out_resized_levels, segmentation_classes)[::-1]\n",
        "    if isinstance(batch_classes, list):\n",
        "      classes_dict = {'prediction': batch_classes.pop()}\n",
        "      for i in range(1, out_resized_levels + 1):\n",
        "        classes_dict['resized_%d' % i] = batch_classes.pop()\n",
        "    else:\n",
        "      classes_dict = {'prediction': batch_classes}\n",
        "    yield batch_imgs, classes_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y5gW0hS7K-zu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "steps_per_epoch= len(train_lists[0]) // batch_size\n",
        "train_gen = imgs_generator(*train_arrays, batch_size, out_resized_levels, segmentation_classes, pre_resized, steps)\n",
        "val_gen = imgs_generator(*val_arrays, batch_size, out_resized_levels, segmentation_classes, pre_resized, steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-T1pn456JmX9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build model"
      ]
    },
    {
      "metadata": {
        "id": "kmb58dH0JWJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d2e7084-dd6b-4733-a572-3d4b5c2b4de0"
      },
      "cell_type": "code",
      "source": [
        "atrous_unet = AtrousUnet(img_shape, filters, segmentation_classes, steps, out_resized_levels,\n",
        "                         kernel_sizes, use_max_pooling=use_max_pooling, use_depthwise=use_depthwise,\n",
        "                         pre_resized=pre_resized, use_regularizers=False)\n",
        "\n",
        "atrous_unet.build_model()\n",
        "print('number of parameters:', atrous_unet.model.count_params())"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of parameters: 9802913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SFwX9lz_wRPf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compile model"
      ]
    },
    {
      "metadata": {
        "id": "sqpRfVCUTf8N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "acafca24-9280-4189-9e9d-e0f6625f5d4f"
      },
      "cell_type": "code",
      "source": [
        "optimizer = Adam(0.0001)\n",
        "loss, metrics, loss_weights = get_loss_metrics_weights(out_resized_levels, use_dice=True, loss_factor=10.)\n",
        "\n",
        "# loss_weights = [loss_factor / out_resized_levels] * out_resized_levels + [loss_factor]\n",
        "\n",
        "\n",
        "atrous_unet.model.compile(optimizer=optimizer, loss=loss,\n",
        "                          metrics=metrics, loss_weights=loss_weights)\n",
        "if colab:\n",
        "    %mkdir {wdir}/logs/"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/semantic_segmentation/logs/’: File exists\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mt0sIeY2TkTj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ]
    },
    {
      "metadata": {
        "id": "KUL4vW7X8GDq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_weight = None  # get_class_weight(segmentation_classes, background_ratio=1/1)\n",
        "weights_path = wdir + '/logs/weights.hdf5'\n",
        "checkpointer = ModelCheckpoint(filepath=weights_path, verbose=1, save_best_only=True)\n",
        "callbacks = [checkpointer]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A_wgDua3oFt3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# preds = atrous_unet.model.predict(np.zeros((5, *img_shape)))\n",
        "# for pr in preds:\n",
        "#   print(pr.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IOqy9baDTpDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cbf65c7c-56d9-466e-8d45-cfcc4279213a"
      },
      "cell_type": "code",
      "source": [
        "history = atrous_unet.model.fit_generator(train_gen,\n",
        "                                          steps_per_epoch=steps_per_epoch, epochs=20,\n",
        "                                          verbose=1, validation_data=val_gen, validation_steps=val_steps,\n",
        "                                          class_weight=class_weight, callbacks=callbacks)\n",
        "\n",
        "with open(wdir+'/logs/history.pkl', 'wb') as handle:\n",
        "    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            " 230/1464 [===>..........................] - ETA: 1:14:50 - loss: -6.6185 - dice_coef: 0.6619 - categorical_accuracy: 0.6634"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WGudDXdwC--I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download results (weights and history)"
      ]
    },
    {
      "metadata": {
        "id": "b0psim7oIYXn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# files.download(wdir+'/logs/weights.hdf5')\n",
        "# files.download(wdir+'/logs/history.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qgloSPnIDIeV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load weights"
      ]
    },
    {
      "metadata": {
        "id": "ptoytQ-rKWlo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "atrous_unet = AtrousUnet(img_shape, filters, segmentation_classes, steps, out_resized_levels,\n",
        "                         kernel_sizes,use_max_pooling=use_max_pooling, use_depthwise=use_depthwise)\n",
        "\n",
        "atrous_unet.build_model()\n",
        "atrous_unet.model.load_weights(wdir + '/logs/weights.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ma4guGx6wDpe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Plot results"
      ]
    },
    {
      "metadata": {
        "id": "yZuwOe1jdTkw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_images_from_predictions(preds):\n",
        "  cmap_dict = get_cmap_dict(reversed=True)\n",
        "  preds = np.argmax(preds, axis=-1)\n",
        "  imgs = np.zeros((*preds.shape, 3))\n",
        "  for i, pred in enumerate(preds):\n",
        "    for j in range(pred.shape[0]):\n",
        "      for k in range(pred.shape[1]):\n",
        "        imgs[i, j, k, :] = cmap_dict[pred[j, k]]\n",
        "  \n",
        "  return imgs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bdCd8P5U2B3O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imgs, labels = val_gen.__next__()\n",
        "predictions = atrous_unet.model.predict_on_batch(imgs)\n",
        "pred_labels = predictions[-1] if len(predictions[0].shape) > 3 else predictions\n",
        "pred_labels = get_images_from_predictions(pred_labels)\n",
        "real_labels = labels['prediction']  # labels[-1] if isinstance(labels, list) and len(labels[0].shape) > 3 else labels\n",
        "real_labels = get_images_from_predictions(real_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D5BbSRp3XdVQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.max(np.argmax(predictions[-1], axis=-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RTFpByDLHiWu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for img, real_label, pred_label in zip(imgs, real_labels, pred_labels):\n",
        "  f, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
        "  ax1.imshow(img)\n",
        "  ax1.axis('off')\n",
        "  ax2.imshow(real_label)\n",
        "  ax2.axis('off')\n",
        "  ax3.imshow(pred_label)\n",
        "  ax3.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MyyysUmf7col",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}